- [Resizual Connection](./resizual_connection.md)
- [Neural Network Models](./Neural_Network_Models.md)
- [applying deep neural networks to natural language processing](./deep_neural_networks_NLP.md)
- [Feature-based vs Fine-tuning](./feature-based_vs_fine-tuning.md)
- [矩阵的“秩”最通俗易懂的解释](./rank.md)

- Activation functions
  - [activation](./activation.md)
  - [relu](./relu.md)
  - [sigmoid](./sigmoid.md)
  - [tanh](./tanh.md)
  - [softmax](./softmax.md)
  - [gelu](./gelu.md)

- Attention
  - [Attention? Attention!](./Attention_%20Attention!%20_%20Lil'Log.pdf)
  - [additive attention vs dot-product attention](./additive%20attention与dot-product%20attention.pdf)
  - [NEURAL MACHINE TRANSLATION BY JOINTLY LEARNING TO ALIGN AND TRANSLATE](./1409.0473.pdf)
  - [Effective Approaches to Attention-based Neural Machine Translation](./1508.04025.pdf)
  - [Attention Is All You Need](./1706.03762.pdf)
  - [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](./1810.04805.pdf)

- [BERT ppt](./bert.md)

- GPT
  - [**GPT-1:** Improving Language Understanding by Generative Pre-Training](./Improving_Language_Understanding_by_Generative_Pre-Training.pdf)
- [gpt-1 vs gpt-2 vs gpt-3](./gpt-1_2_3.md)

- Fine Tuning
  - [通俗解读大模型微调(Fine Tuning)](./fine_tuning.md)
  - [大模型低秩适配器LoRA](./lora.md)

- Other papers
  - [GLM](./GLM-2103.10360.pdf)
  - [The_NLP_Cookbook_Modern_Recipes_for_Transformer](./The_NLP_Cookbook_Modern_Recipes_for_Transformer_Ba.pdf)
  - [A_Review_of_Deep_Contextualized_Word_Representations](./A_Review_of_Deep_Contextualized_Word_Representations.md)
  - [Lora](./2106.09685.pdf)
  - [QLora](./2305.14314.pdf)
  - [A Comprehensive Survey of AI-Generated Content (AIGC): A History of Generative AI from GAN to ChatGPT](./2303.04226.pdf)
